{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1030 15:29:58.193505 4481746368 __init__.py:43] Loading faiss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n",
      "1.7.0\n",
      "1.6.1\n"
     ]
    }
   ],
   "source": [
    "import transformers, torch, faiss\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from mips import MIPS\n",
    "\n",
    "#check version\n",
    "print (transformers.__version__)\n",
    "print (torch.__version__)\n",
    "print (faiss.__version__)\n",
    "\n",
    "#Load GPT2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', return_dict=True)\n",
    "model = model.eval()\n",
    "\n",
    "\n",
    "#Set datastore\n",
    "STORE_FILE='keys.npy'\n",
    "MAXIMUM_SIZE=10000\n",
    "DIMENSION=768\n",
    "all_keys = np.memmap(STORE_FILE, dtype=np.float32, mode='w+', shape=(MAXIMUM_SIZE, DIMENSION))\n",
    "finished_keys = 0\n",
    "\n",
    "TOKEN_FILE='tokens.npy'\n",
    "all_tokens = np.memmap(TOKEN_FILE, dtype=np.int, mode='w+', shape=(MAXIMUM_SIZE,))\n",
    "all_lengths = []\n",
    "finished_tokens = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [\"My dog is cute\", \"My idea is brilliant.\", \"My paper is very very good!\"]\n",
    "data = [batch] * 100\n",
    "# encode data\n",
    "for batch in data:\n",
    "    inputs = tokenizer(batch,\n",
    "                       padding=True,\n",
    "                       return_length=True,\n",
    "                       return_tensors=\"pt\")\n",
    "    assert (inputs['length'] > 1).all()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=inputs['input_ids'],\n",
    "                        attention_mask = inputs['attention_mask'],\n",
    "                        output_hidden_states=True)\n",
    "        # We pick the hidden state at the last layer as the key\n",
    "        keys = outputs['hidden_states'][-1]\n",
    "        bsz, seq_len, dim = keys.shape\n",
    "        for i in range(bsz):\n",
    "            len_i = inputs['length'][i]\n",
    "            all_keys[finished_keys:finished_keys+len_i-1] = keys[i,:len_i-1] # we do not need the last key \n",
    "            all_tokens[finished_tokens:finished_tokens+len_i] = inputs['input_ids'][i,:len_i]\n",
    "            finished_keys += (len_i -1)\n",
    "            finished_tokens += len_i\n",
    "            all_lengths.extend(inputs['length'].tolist())\n",
    "    #print ('finished_keys', finished_keys, 'finished_tokens', finished_tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make index\n",
    "INDEX_TYPE = \"IVF10_HNSW32,SQ8\" # change it to 'IVF4096_HNSW32,SQ8' for actual use\n",
    "mips = MIPS(DIMENSION, INDEX_TYPE, efSearch=128, nprobe=64)\n",
    "mips.train(all_keys[:finished_keys])\n",
    "mips.add(all_keys[:finished_keys])\n",
    "cumsum_keys = np.cumsum(np.array(all_lengths)-1)\n",
    "\n",
    "# This function is used to return corresponding sentence and word\n",
    "def find_in_corpus(idx):\n",
    "    if idx < cumsum_keys[0]:\n",
    "        sent_idx = 0\n",
    "        sent_start = 0\n",
    "        sent_end = cumsum_keys[0] + 1\n",
    "        word_pos = idx + 1\n",
    "    else:\n",
    "        sent_idx = np.searchsorted(cumsum_keys, idx, side='right') #cumsum_keys[i-1] <= v < cumsum_keys[i]\n",
    "        sent_end = cumsum_keys[sent_idx] + sent_idx + 1  \n",
    "        sent_start = sent_end - (cumsum_keys[sent_idx] - cumsum_keys[sent_idx-1] + 1)\n",
    "        word_pos = idx - cumsum_keys[sent_idx-1] + 1\n",
    "    sent = all_tokens[sent_start:sent_end]\n",
    "    word = sent[word_pos]\n",
    "    return tokenizer.decode(sent), tokenizer.decode([word]), word_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 0, distance 0.00, sent 0: My dog is cute, next word position 1:  dog\n",
      "rank 1, distance 0.00, sent 7: My paper is very very good!, next word position 1:  paper\n",
      "rank 2, distance 0.00, sent 3: My idea is brilliant., next word position 1:  idea\n"
     ]
    }
   ],
   "source": [
    "#test any prefix\n",
    "topk = 3 # topk search\n",
    "inputs = tokenizer(\"My\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=inputs['input_ids'],\n",
    "                    attention_mask = inputs['attention_mask'],\n",
    "                    output_hidden_states=True)\n",
    "    # We pick the hidden state at the last layer as the key\n",
    "    keys = outputs['hidden_states'][-1]\n",
    "    bsz, seq_len, dim = keys.shape\n",
    "    search_key = keys[0,-1].numpy()\n",
    "    D, I = mips.search(np.array([search_key]), topk)\n",
    "    # dis is \n",
    "    for rnk, (idx, dist) in enumerate(zip(I[0], D[0])):\n",
    "        sent, word, word_pos = find_in_corpus(idx)\n",
    "        print(\"rank %d, distance %.2f, sent %d: %s, next word position %d: %s\"%(rnk, dist, idx, sent, word_pos, word))\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
